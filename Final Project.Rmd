---
title: "Final Project"
aithor: "Sangsoo Seo"
date: '2022-06-09'

output: 
  html_document:
    theme: yeti
    toc: yes
    toc_float: yes
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include=FALSE}
install.packages('tidyverse', repos = "http://cran.us.r-project.org")
#install.packages('tidymodels', repos = "http://cran.us.r-project.org")
install.packages('ggplot2', repos = "http://cran.us.r-project.org")
install.packages('visdat', repos = "http://cran.us.r-project.org")
install.packages("corrplot", repos = "http://cran.us.r-project.org")
install.packages("discrim", repos = "http://cran.us.r-project.org")
install.packages("rlang", repos = "http://cran.us.r-project.org")
install.packages('janitor', repos = "http://cran.us.r-project.org")
install.packages('rpart.plot', repos = "http://cran.us.r-project.org")
install.packages('ranger', repos = "http://cran.us.r-project.org")
install.packages('vip', repos = "http://cran.us.r-project.org")
install.packages('xgboost', repos = "http://cran.us.r-project.org")
library(tidymodels)
library(ggplot2)
library(visdat)
library(corrplot)
library(discrim)
library(klaR)
library(yardstick)
library(janitor)
library(rpart.plot)
library(ranger)
library(vip)
library(xgboost)
```


# Introduction
$\\$

The purpose of this project is to find out what factors make the evaluation of wine high, and ultimately pass on information to those who do not know much about wine as a result of this. This is prediction project and 'quality' variable is the target variable. 



## Data Cleaning

* Clean names
```{r}
data = read.csv('wine.csv')
data = data %>% clean_names()
```


* Check NAs
```{r}
vis_dat(data)
```
        + There is no NA value.
        + All numeric variables. 
        

* Split data
```{r}
data_split <- data %>% 
        initial_split(prop = 0.7)

train <- training(data_split)
test <- testing(data_split)
```
        + This is to predict 'quality' value, so I do not need to add 'strata' argument. 



## Exploratory Data Analysis

* correlation
```{r}
M = cor(data)
corrplot(M, method = 'color', col = COL2(n=20), cl.length = 21, order = 'AOE',
         addCoef.col = 'grey')
```
        + 'free_sulfur_dioxide' and 'total_sulfur_dioxide' seem similar. 
        + 'acidity' variables also seem similar as sulfur dioxide. 
        + 'alcohol' is the highest correlated variable with the target variable, 'quality'.


* Plot
```{r}
train %>% ggplot(aes(x = as.factor(quality), y = alcohol)) +
        geom_boxplot()
```
        + As I saw, 'quality' and 'alcohol' are positivly correlated. 

```{r}
train %>% ggplot(aes(x = as.factor(quality), y = volatile_acidity)) +
        geom_boxplot()
```
        + On the other hand, 'volatile_acidity' is negatively correlated. 

```{r}

```


## Model Building
* CV folds
```{r}
folds <- vfold_cv(train, v = 5)
```
        + I divided the train set to 5 folds.


* Recipe
```{r}
recipe = recipe(quality ~ ., data = train) %>%
        step_normalize(all_predictors())
```


* Decision Tree
```{r}
tree_model = decision_tree(cost_complexity = tune()) %>%
        set_engine('rpart') %>%
        set_mode('regression')

tree_wf = workflow() %>%
        add_model(tree_model) %>%
        add_recipe(recipe)

tree_grid <- grid_regular(cost_complexity(range = c(-3, -1)), 
                          levels = 2)

tree_tune <- tree_wf %>% 
        tune_grid(resamples = folds, 
        grid = tree_grid)

save(tree_tune, tree_wf, file = "tree_tune.rda")

autoplot(tree_tune)
```


* Random Forest
```{r, message=FALSE}
rf_model = rand_forest(min_n = tune(),
                       mtry = tune(),
                       mode = "regression") %>% 
        set_engine("ranger")

rf_wf = workflow() %>% 
        add_model(rf_model) %>% 
        add_recipe(recipe)

rf_params = parameters(rf_model) %>% 
        update(mtry = mtry(range= c(2, 120)))

rf_grid = grid_regular(rf_params, 
                       levels = 2)

rf_tune = rf_wf %>% 
        tune_grid(resamples = folds, 
                  grid = rf_grid,
                  metrics = metric_set(rmse, rsq))

save(rf_tune, rf_wf, file = "rf_tune.rda")

autoplot(rf_tune)
```


* Boosted Tree
```{r, message=FALSE}
bt_model = boost_tree(min_n = tune(),
                      mtry = tune(),
                      learn_rate = tune(),
                      mode = "regression") %>% 
        set_engine("xgboost")

bt_wf = workflow() %>% 
        add_model(bt_model) %>% 
        add_recipe(recipe)

bt_params = parameters(bt_model) %>% 
        update(mtry = mtry(range= c(2, 120)),
               learn_rate = learn_rate(range = c(-5, 0.2)))

bt_grid = grid_regular(bt_params, levels = 2)

bt_tune = bt_wf %>% 
        tune_grid(resamples = folds, 
                  grid = bt_grid,
                  metrics = metric_set(rmse, rsq))

save(bt_tune, bt_wf, file = "bt_tune.rda")

autoplot(bt_tune)
```


* Test set Prediction 
```{r, message=FALSE, error=FALSE}
final_wf <- rf_workflow %>% 
  finalize_workflow(select_best(rf_tune, metric = "rmse"))

wine_metric <- metric_set(rmse)

test_predictions <- predict(final_model, new_data = test) %>% 
  bind_cols(test %>% select(difficulty)) 

model_test_predictions_type <- predict(final_model, new_data = test) %>% 
  bind_cols(test %>% select(difficulty, type)) 

model_test_predictions %>% 
  peloton_metric(truth = difficulty, estimate = .pred)
```


## Conclusion
Random forest is the best model for prediction. The higher the complexity, the higher $R^2$ and it indicates the model is explaining well. However, comprehensive results are quite disappointing. I should try to make a derived variable or other things to make a model works well. 



